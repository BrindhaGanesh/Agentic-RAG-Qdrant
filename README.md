# Agentic RAG Medical Chatbot (Qdrant + Chainlit)

This project is an **Agentic Retrieval-Augmented Generation (RAG)** chatbot designed to answer medical questions using curated datasets and intelligent retrieval. It combines **Qdrant vector search**, **LangGraph agent workflows**, **Sentence Transformer embeddings**, **Chainlit UI**, and **OpenAI models** to deliver concise, context-aware responses.

This chatbot is deployed on **Render**, using **environment variables for security**, and retrieves information from vectorized Kaggle medical datasets stored in **Qdrant Cloud**.
 

---

## ğŸŒŸ What the App Does

- Accepts a userâ€™s medical or device-related question  
- Routes the question to the appropriate data source using an **agentic router**  
- Retrieves relevant context using **semantic search via Qdrant**  
- Checks whether the retrieved context is relevant  
- Falls back to **web search** (Tavily) if needed  
- Builds an optimized RAG prompt  
- Generates a short, reliable answer using an **OpenAI LLM**

The chatbot uses real medical datasets (manuals + Q&A) to answer questions with high-quality, dataset-grounded context.

---

## ğŸ§  How the System Works

### 1ï¸âƒ£ Agentic Router (LangGraph)
Determines the best information source based on the user query:

- **Medical Q&A dataset**
- **Medical Device Manuals dataset**
- **Web search** (if the query is outside dataset scope)

### 2ï¸âƒ£ Vector Retrieval (Qdrant Cloud)
All dataset entries are converted into embeddings using a **Sentence Transformer** model and stored in Qdrant.

For each query:

- The question is embedded  
- Qdrant returns the most similar documents  
- These documents become the RAG context  

### 3ï¸âƒ£ Context Relevance Check
An LLM checks:

> â€œIs this retrieved context actually relevant to the question?â€

If **not relevant**, the workflow retries or uses web search.

### 4ï¸âƒ£ Prompt Construction
A final prompt is built using:

- Retrieved context  
- User question  
- Safety and brevity instructions  

### 5ï¸âƒ£ Answer Generation
An OpenAI model generates a ~50-word answer based on combined context and prompt instructions.

---

## ğŸ› ï¸ Technologies Used

### **ğŸ”¹ Qdrant (Vector Database)**
- Stores embeddings for both Kaggle datasets  
- Provides high-speed semantic search with `query_points`

### **ğŸ”¹ Sentence Transformers**
- Model: `all-MiniLM-L6-v2`  
- Converts text into dense embeddings

### **ğŸ”¹ Kaggle Datasets**
Two curated public datasets:

- Global Medical Device Manuals  
- Comprehensive Medical Q&A dataset  

Downloaded through Kaggle API â†’ processed â†’ embedded â†’ uploaded to Qdrant.

### **ğŸ”¹ LangGraph (Agent Workflow Engine)**
Implements:

- Router agent  
- Relevance agent  
- Prompt builder agent  
- LLM generator agent  

### **ğŸ”¹ Chainlit**
- Provides a clean, interactive web-based chat UI  
- Handles real-time conversation with the RAG pipeline

### **ğŸ”¹ OpenAI API**
- Used for routing, relevance checking, and final answer generation

### **ğŸ”¹ Render (Hosting)**
- Hosts the Chainlit application  
- Environment variables store API keys securely

---

 
